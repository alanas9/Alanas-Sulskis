{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "### Practical Session\n",
    "\n",
    "<br/> Prof. Dr. Georgios K. Ouzounis\n",
    "<br/> email: georgios.ouzounis@go.kauko.lt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. Data loading and pre-processing\n",
    "2. Building the RNN\n",
    "3. Train and deploy the RNN\n",
    "4. Improving the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.sapientrade.com/images/2017/02/14/AI-Machine-Learning-Trading-Benefits.jpg\" width=\"800\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a 5-year history of the Google Stock prices predict the stock values for the period of the recent most month that are not included in the historical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data-sets\n",
    "\n",
    "The data-sets are two comma-separated values files (CSV) and contain a data table of 1258 records for training and a table of 21 records for testing.\n",
    "\n",
    "They can be found at the [Kaggle.com website](https://www.kaggle.com/akram24/google-stock-price-train) or at various web locations after searching for the filenames: \n",
    "\n",
    "**Google_Stock_Price_Test.csv** and **Google_Stock_Price_Train.csv**\n",
    "\n",
    "Known alternative location: Github user [pdway53](https://github.com/pdway53/Predict_Google_Stock_Price_RNN) \n",
    "\n",
    "Open a terminal and use the wget command to get it of the selected location. Example:\n",
    "\n",
    "```shell\n",
    "wget https://raw.githubusercontent.com/pdway53/Predict_Google_Stock_Price_RNN/master/Google_Stock_Price_Test.csv\n",
    "\n",
    "wget https://raw.githubusercontent.com/pdway53/Predict_Google_Stock_Price_RNN/master/Google_Stock_Price_Train.csv \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n",
    "\n",
    "We need 3 main libraries:\n",
    "\n",
    "- [Numpy](http://www.numpy.org): it is the fundamental package for scientific computing with Python. It contains among other things a powerful N-dimensional array object that can be used as an efficient multi-dimensional container of generic data. Arbitrary data-types can be defined.\n",
    "- [matplotlib](https://matplotlib.org):  it is a Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms.\n",
    "- [pandas](https://pandas.pydata.org): is a software library written for the Python programming language for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset description: the open high, low and close values of the Google Stock from 2012 to 2016. [Relevant code here](https://github.com/pdway53/Predict_Google_Stock_Price_RNN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the dataset\n",
    "\n",
    "# load the file contents \n",
    "dataset_train = pd.read_csv('training_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7/1/2009</td>\n",
       "      <td>5.125000</td>\n",
       "      <td>5.166429</td>\n",
       "      <td>5.090000</td>\n",
       "      <td>5.101071</td>\n",
       "      <td>4.408037</td>\n",
       "      <td>414178800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7/2/2009</td>\n",
       "      <td>5.044643</td>\n",
       "      <td>5.101071</td>\n",
       "      <td>4.992500</td>\n",
       "      <td>5.000714</td>\n",
       "      <td>4.321311</td>\n",
       "      <td>370479200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7/6/2009</td>\n",
       "      <td>4.953571</td>\n",
       "      <td>4.963929</td>\n",
       "      <td>4.866071</td>\n",
       "      <td>4.950357</td>\n",
       "      <td>4.277796</td>\n",
       "      <td>498688400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7/7/2009</td>\n",
       "      <td>4.945714</td>\n",
       "      <td>4.988572</td>\n",
       "      <td>4.827857</td>\n",
       "      <td>4.835714</td>\n",
       "      <td>4.178730</td>\n",
       "      <td>461596800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7/8/2009</td>\n",
       "      <td>4.854286</td>\n",
       "      <td>4.930000</td>\n",
       "      <td>4.800714</td>\n",
       "      <td>4.900714</td>\n",
       "      <td>4.234900</td>\n",
       "      <td>575929200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date      Open      High       Low     Close  Adj Close     Volume\n",
       "0  7/1/2009  5.125000  5.166429  5.090000  5.101071   4.408037  414178800\n",
       "1  7/2/2009  5.044643  5.101071  4.992500  5.000714   4.321311  370479200\n",
       "2  7/6/2009  4.953571  4.963929  4.866071  4.950357   4.277796  498688400\n",
       "3  7/7/2009  4.945714  4.988572  4.827857  4.835714   4.178730  461596800\n",
       "4  7/8/2009  4.854286  4.930000  4.800714  4.900714   4.234900  575929200"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subtable of relevant entries (open values)\n",
    "# The .values makes this vector a numpy array\n",
    "training_set = dataset_train.iloc[:, 1:2].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.125   ],\n",
       "       [ 5.044643],\n",
       "       [ 4.953571],\n",
       "       ...,\n",
       "       [51.869999],\n",
       "       [52.115002],\n",
       "       [52.189999]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a subset to train a model.\n",
    "training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy arrays do not support the view() and head() methods. [More on accessing the numpy data](https://jakevdp.github.io/PythonDataScienceHandbook/02.02-the-basics-of-numpy-arrays.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "Next we need to rescale our data to the range from 0 to 1. \n",
    "\n",
    "Feature scaling is essential as discussed if the Features lecture and needs to be applied to both the training and test sets.\n",
    "\n",
    "It is computed using the ScikitLearn library [MinMaxScaler()](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler) which transforms the selected feature by scaling it to a given range. If more than one, this estimator scales and translates each feature individually such that it is in the given range on the training set, i.e. between zero and one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "\n",
    "# import the MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scaler instance to rescale all data to the range of 0.0 to 1.0 \n",
    "sc = MinMaxScaler(feature_range = (0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the actual training set of scaled values\n",
    "training_set_scaled = sc.fit_transform(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00512321],\n",
       "       [0.00360247],\n",
       "       [0.00187895],\n",
       "       ...,\n",
       "       [0.88976301],\n",
       "       [0.89439965],\n",
       "       [0.89581895]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the training set to dependent and independent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1bckuLGZCeLUzNA-xJCGOODzC-4n2U-If\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data structure with 90 timesteps and 1 output\n",
    "\n",
    "# the 90 stock prices in the last 3 months before today\n",
    "X_train = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2537, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the stock price today\n",
    "y_train = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we start from day 90 because that is the first instance allowing us to \n",
    "# go back 90 days\n",
    "for i in range(90, 1258): \n",
    "    # 0 is the column ID, the only column in this case.    \n",
    "    # put the last 60 days values in one row of X_train\n",
    "    X_train.append(training_set_scaled[i-90:i, 0]) \n",
    "    y_train.append(training_set_scaled[i, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = np.array(X_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00512321, 0.00360247, 0.00187895, ..., 0.03509875, 0.0370453 ,\n",
       "        0.03817401],\n",
       "       [0.00360247, 0.00187895, 0.00173026, ..., 0.0370453 , 0.03817401,\n",
       "        0.03824837],\n",
       "       [0.00187895, 0.00173026, 0.        , ..., 0.03817401, 0.03824837,\n",
       "        0.04124254],\n",
       "       ...,\n",
       "       [0.27716722, 0.27632236, 0.26837395, ..., 0.34018681, 0.33749003,\n",
       "        0.33493518],\n",
       "       [0.27632236, 0.26837395, 0.26823877, ..., 0.33749003, 0.33493518,\n",
       "        0.33569219],\n",
       "       [0.26837395, 0.26823877, 0.26172324, ..., 0.33493518, 0.33569219,\n",
       "        0.33782121]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping the Matrix\n",
    "\n",
    "We need to add a new matrix dimension to accommodate the indicator (predictor). \n",
    "\n",
    "NumPy matrices are tensors (3D) and essentially we need to specify that our matrix consists of **60 days** (dimension x) times **total days in data set** (dimension y) times **1 value per matrix cell (scalar)** (dimension z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to add the stock value of somebody else together with the the past 60 days of Google, we need to change the length of the 3 dimension to  2.  RNN training tables are 3D!!! Read: [Reshaping NumPy Array | Numpy Array Reshape Examples](https://backtobazics.com/python/python-reshaping-numpy-array-examples/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the data matrix, we retain the 2 original dimensions and add a third of depth=1\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN initialization\n",
    "\n",
    "- Import the sequential model from the Keras API;\n",
    "- Import the Dense layer template from the Keras API;\n",
    "- Import the LSTM model from the Keras API\n",
    "- Create an instance of the sequential model called regressor because we want to predict a continuous value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the RNN as a sequence of layers\n",
    "regressor = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add First Layer\n",
    "\n",
    "We first add an object of the LSTM class! \n",
    "\n",
    "- The first argument is the number of units or LSTM memory cells. Include many neurons to address the high dimensionality of the problem; say 50 neurons! \n",
    "- Second arg: return sequences = true; stacked LSTM !\n",
    "- Third arg: input 3D shape: observations vs time steps vs number of indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the input layer and the LSTM layer\n",
    "regressor.add(LSTM(units = 50, return_sequences = True, input_shape =  (X_train.shape[1], 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the argument is the dropout rate to ignore in the layers (20%), \n",
    "# i.e. 50 units * 20% = 10 units will be dropped each time\n",
    "regressor.add(Dropout(0.2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add More Layers\n",
    "\n",
    "We can add more LSTM layers but along with Dropout regularization to make sure we avoid overfitting! \n",
    "\n",
    "We don’t need to add the shape of the layer again because it is recognized automatically from the number of input units.\n",
    "\n",
    "The last layer does not return a sequence but connected directly to a fully connected output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a second LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a third LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a fourth LSTM layer and some Dropout regularisation\n",
    "# we removed the return_sequences because we no longer return a \n",
    "# sequence but a value instead\n",
    "regressor.add(LSTM(units = 50))\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Output Layer & Compile\n",
    "\n",
    "The output has 1 dimension , i.e. one value to be predicted thus or output fully connected layer has dimensionality = 1.\n",
    "\n",
    "- **Optimizer**: rmsprop is recommended in the Keras documentation. The Adam optimizer is also a powerful choice.\n",
    "- **Loss function**: regression problems take the mean square error as most common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the output layer\n",
    "regressor.add(Dense(units = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the RNN\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and deploy the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the RNN to the Training set\n",
    "\n",
    "We now want to train our RNN using the data in our **Training Set X** and **predictors in y** (ground truth in this case). Parameters that can be specified are the:\n",
    "\n",
    "- **Batch size**:  update the cell weights not on every stock price on every batch_size values; \n",
    "- **Number of epochs**: how many iterations to be used, i.e. number of forward and backward propagations for the update of the weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "37/37 [==============================] - 8s 212ms/step - loss: 1.5655e-04\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 1.5037e-04\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 8s 221ms/step - loss: 1.5366e-04\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 7s 186ms/step - loss: 1.5306e-04\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 8s 211ms/step - loss: 1.5848e-04\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 8s 211ms/step - loss: 1.5206e-04\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 7s 191ms/step - loss: 1.4984e-04\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 7s 181ms/step - loss: 1.3128e-04\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 7s 181ms/step - loss: 1.4657e-04\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 7s 188ms/step - loss: 1.5566e-04\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 9s 237ms/step - loss: 1.7151e-04\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 8s 224ms/step - loss: 1.7244e-04\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 8s 210ms/step - loss: 1.6642e-04\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 8s 207ms/step - loss: 1.4360e-04\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 8s 209ms/step - loss: 1.5016e-04\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 9s 255ms/step - loss: 1.6231e-04\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 7s 192ms/step - loss: 1.4383e-04\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 8s 217ms/step - loss: 1.5886e-04\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 8s 229ms/step - loss: 1.8173e-04\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 8s 210ms/step - loss: 1.5364e-04\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 8s 223ms/step - loss: 1.5308e-04\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 8s 215ms/step - loss: 1.6288e-04\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 7s 197ms/step - loss: 1.3579e-04\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 8s 211ms/step - loss: 1.7289e-04\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 9s 235ms/step - loss: 1.5814e-04\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 8s 220ms/step - loss: 1.4520e-04\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 8s 212ms/step - loss: 1.4439e-04\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 8s 220ms/step - loss: 1.2741e-04\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 6s 175ms/step - loss: 1.4347e-04\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 8s 210ms/step - loss: 1.3137e-04\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - 10s 276ms/step - loss: 1.4543e-04\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - 8s 204ms/step - loss: 1.8117e-04\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - 8s 229ms/step - loss: 1.4743e-04\n",
      "Epoch 34/100\n",
      "37/37 [==============================] - 10s 263ms/step - loss: 1.4128e-04\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - 7s 194ms/step - loss: 1.8184e-04\n",
      "Epoch 36/100\n",
      "37/37 [==============================] - 7s 189ms/step - loss: 1.4859e-04\n",
      "Epoch 37/100\n",
      "37/37 [==============================] - 8s 204ms/step - loss: 1.3693e-04\n",
      "Epoch 38/100\n",
      "37/37 [==============================] - 8s 228ms/step - loss: 1.4321e-04\n",
      "Epoch 39/100\n",
      "37/37 [==============================] - 7s 199ms/step - loss: 1.5081e-04\n",
      "Epoch 40/100\n",
      "37/37 [==============================] - 9s 252ms/step - loss: 1.3550e-04\n",
      "Epoch 41/100\n",
      "37/37 [==============================] - 7s 194ms/step - loss: 1.3276e-04\n",
      "Epoch 42/100\n",
      "37/37 [==============================] - 9s 254ms/step - loss: 1.6097e-04\n",
      "Epoch 43/100\n",
      "37/37 [==============================] - 8s 217ms/step - loss: 1.4532e-04\n",
      "Epoch 44/100\n",
      "37/37 [==============================] - 8s 205ms/step - loss: 1.3170e-04\n",
      "Epoch 45/100\n",
      "37/37 [==============================] - 7s 194ms/step - loss: 1.6366e-04\n",
      "Epoch 46/100\n",
      "37/37 [==============================] - 8s 227ms/step - loss: 1.2803e-04\n",
      "Epoch 47/100\n",
      "37/37 [==============================] - 8s 227ms/step - loss: 1.3171e-04\n",
      "Epoch 48/100\n",
      "37/37 [==============================] - 8s 216ms/step - loss: 1.2216e-04\n",
      "Epoch 49/100\n",
      "37/37 [==============================] - 8s 222ms/step - loss: 1.5817e-04\n",
      "Epoch 50/100\n",
      "37/37 [==============================] - 8s 211ms/step - loss: 1.8308e-04\n",
      "Epoch 51/100\n",
      "37/37 [==============================] - 7s 194ms/step - loss: 1.4132e-04\n",
      "Epoch 52/100\n",
      "37/37 [==============================] - 7s 198ms/step - loss: 1.2706e-04\n",
      "Epoch 53/100\n",
      "37/37 [==============================] - 8s 205ms/step - loss: 1.3725e-04\n",
      "Epoch 54/100\n",
      "37/37 [==============================] - 9s 230ms/step - loss: 1.2663e-04\n",
      "Epoch 55/100\n",
      "37/37 [==============================] - 7s 198ms/step - loss: 1.2633e-04\n",
      "Epoch 56/100\n",
      "37/37 [==============================] - 8s 219ms/step - loss: 1.3744e-04\n",
      "Epoch 57/100\n",
      "37/37 [==============================] - 10s 282ms/step - loss: 1.3108e-04\n",
      "Epoch 58/100\n",
      "37/37 [==============================] - 8s 217ms/step - loss: 1.3895e-04\n",
      "Epoch 59/100\n",
      "37/37 [==============================] - 9s 246ms/step - loss: 1.3417e-04\n",
      "Epoch 60/100\n",
      "37/37 [==============================] - 8s 204ms/step - loss: 1.3864e-04\n",
      "Epoch 61/100\n",
      "37/37 [==============================] - 9s 235ms/step - loss: 1.3128e-04\n",
      "Epoch 62/100\n",
      "37/37 [==============================] - 8s 217ms/step - loss: 1.5078e-04\n",
      "Epoch 63/100\n",
      "37/37 [==============================] - 8s 212ms/step - loss: 1.2654e-04\n",
      "Epoch 64/100\n",
      "37/37 [==============================] - 9s 234ms/step - loss: 1.1565e-04\n",
      "Epoch 65/100\n",
      "37/37 [==============================] - 7s 200ms/step - loss: 1.3463e-04\n",
      "Epoch 66/100\n",
      "37/37 [==============================] - 8s 207ms/step - loss: 1.2246e-04\n",
      "Epoch 67/100\n",
      "37/37 [==============================] - 8s 207ms/step - loss: 1.2708e-04\n",
      "Epoch 68/100\n",
      "37/37 [==============================] - 8s 207ms/step - loss: 1.5748e-04\n",
      "Epoch 69/100\n",
      "37/37 [==============================] - 8s 228ms/step - loss: 1.5085e-04\n",
      "Epoch 70/100\n",
      "37/37 [==============================] - 8s 206ms/step - loss: 1.2421e-04\n",
      "Epoch 71/100\n",
      "37/37 [==============================] - 8s 217ms/step - loss: 1.2925e-04\n",
      "Epoch 72/100\n",
      "37/37 [==============================] - 8s 212ms/step - loss: 1.2585e-04\n",
      "Epoch 73/100\n",
      "37/37 [==============================] - 7s 191ms/step - loss: 1.3301e-04\n",
      "Epoch 74/100\n",
      "37/37 [==============================] - 8s 205ms/step - loss: 1.2820e-04\n",
      "Epoch 75/100\n",
      "37/37 [==============================] - 9s 230ms/step - loss: 1.3511e-04\n",
      "Epoch 76/100\n",
      "37/37 [==============================] - 8s 212ms/step - loss: 1.4643e-04\n",
      "Epoch 77/100\n",
      "37/37 [==============================] - 8s 218ms/step - loss: 1.3500e-04\n",
      "Epoch 78/100\n",
      "37/37 [==============================] - 7s 199ms/step - loss: 1.2337e-04\n",
      "Epoch 79/100\n",
      "37/37 [==============================] - 7s 188ms/step - loss: 1.1574e-04\n",
      "Epoch 80/100\n",
      "37/37 [==============================] - 6s 175ms/step - loss: 1.1836e-04\n",
      "Epoch 81/100\n",
      "37/37 [==============================] - 8s 214ms/step - loss: 1.2533e-04\n",
      "Epoch 82/100\n",
      "37/37 [==============================] - 7s 191ms/step - loss: 1.3791e-04\n",
      "Epoch 83/100\n",
      "37/37 [==============================] - 8s 214ms/step - loss: 1.1501e-04\n",
      "Epoch 84/100\n",
      "37/37 [==============================] - 8s 224ms/step - loss: 1.3289e-04\n",
      "Epoch 85/100\n",
      "37/37 [==============================] - 10s 278ms/step - loss: 1.3646e-04\n",
      "Epoch 86/100\n",
      "37/37 [==============================] - 8s 227ms/step - loss: 1.5108e-04\n",
      "Epoch 87/100\n",
      "37/37 [==============================] - 9s 240ms/step - loss: 1.6019e-04\n",
      "Epoch 88/100\n",
      "37/37 [==============================] - 7s 198ms/step - loss: 1.2091e-04\n",
      "Epoch 89/100\n",
      "37/37 [==============================] - 8s 222ms/step - loss: 1.3192e-04\n",
      "Epoch 90/100\n",
      "37/37 [==============================] - 8s 224ms/step - loss: 1.1798e-04\n",
      "Epoch 91/100\n",
      "37/37 [==============================] - 9s 230ms/step - loss: 1.3208e-04\n",
      "Epoch 92/100\n",
      "37/37 [==============================] - 8s 208ms/step - loss: 1.4399e-04\n",
      "Epoch 93/100\n",
      "37/37 [==============================] - 8s 229ms/step - loss: 1.2616e-04\n",
      "Epoch 94/100\n",
      "37/37 [==============================] - 8s 214ms/step - loss: 1.1749e-04\n",
      "Epoch 95/100\n",
      "37/37 [==============================] - 8s 206ms/step - loss: 1.2575e-04\n",
      "Epoch 96/100\n",
      "37/37 [==============================] - 8s 210ms/step - loss: 1.3935e-04\n",
      "Epoch 97/100\n",
      "37/37 [==============================] - 8s 223ms/step - loss: 1.5881e-04\n",
      "Epoch 98/100\n",
      "37/37 [==============================] - 9s 244ms/step - loss: 1.2329e-04\n",
      "Epoch 99/100\n",
      "37/37 [==============================] - 8s 218ms/step - loss: 1.2709e-04\n",
      "Epoch 100/100\n",
      "37/37 [==============================] - 8s 230ms/step - loss: 1.4289e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9e00553a30>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the RNN to the Training set\n",
    "regressor.fit(X_train, y_train, epochs = 100, batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Predictions\n",
    "\n",
    "Create a data-frame by importing the Google Stock Price Test set for January 2017 using pandas and make it a numpy array.\n",
    "\n",
    "There are 20 financial days in one month, weekends are excluded!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8/1/2019</td>\n",
       "      <td>53.474998</td>\n",
       "      <td>54.507500</td>\n",
       "      <td>51.685001</td>\n",
       "      <td>52.107498</td>\n",
       "      <td>51.400280</td>\n",
       "      <td>216071600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8/2/2019</td>\n",
       "      <td>51.382500</td>\n",
       "      <td>51.607498</td>\n",
       "      <td>50.407501</td>\n",
       "      <td>51.005001</td>\n",
       "      <td>50.312748</td>\n",
       "      <td>163448400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8/5/2019</td>\n",
       "      <td>49.497501</td>\n",
       "      <td>49.662498</td>\n",
       "      <td>48.145000</td>\n",
       "      <td>48.334999</td>\n",
       "      <td>47.678982</td>\n",
       "      <td>209572000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8/6/2019</td>\n",
       "      <td>49.077499</td>\n",
       "      <td>49.517502</td>\n",
       "      <td>48.509998</td>\n",
       "      <td>49.250000</td>\n",
       "      <td>48.581562</td>\n",
       "      <td>143299200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8/7/2019</td>\n",
       "      <td>48.852501</td>\n",
       "      <td>49.889999</td>\n",
       "      <td>48.455002</td>\n",
       "      <td>49.759998</td>\n",
       "      <td>49.084641</td>\n",
       "      <td>133457600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8/8/2019</td>\n",
       "      <td>50.049999</td>\n",
       "      <td>50.882500</td>\n",
       "      <td>49.847500</td>\n",
       "      <td>50.857498</td>\n",
       "      <td>50.167248</td>\n",
       "      <td>108038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8/9/2019</td>\n",
       "      <td>50.325001</td>\n",
       "      <td>50.689999</td>\n",
       "      <td>49.822498</td>\n",
       "      <td>50.247501</td>\n",
       "      <td>49.753849</td>\n",
       "      <td>98478800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8/12/2019</td>\n",
       "      <td>49.904999</td>\n",
       "      <td>50.512501</td>\n",
       "      <td>49.787498</td>\n",
       "      <td>50.119999</td>\n",
       "      <td>49.627602</td>\n",
       "      <td>89927600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8/13/2019</td>\n",
       "      <td>50.255001</td>\n",
       "      <td>53.035000</td>\n",
       "      <td>50.119999</td>\n",
       "      <td>52.242500</td>\n",
       "      <td>51.729252</td>\n",
       "      <td>188874000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8/14/2019</td>\n",
       "      <td>50.790001</td>\n",
       "      <td>51.610001</td>\n",
       "      <td>50.647499</td>\n",
       "      <td>50.687500</td>\n",
       "      <td>50.189526</td>\n",
       "      <td>146189600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8/15/2019</td>\n",
       "      <td>50.865002</td>\n",
       "      <td>51.285000</td>\n",
       "      <td>49.917500</td>\n",
       "      <td>50.435001</td>\n",
       "      <td>49.939507</td>\n",
       "      <td>108909600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8/16/2019</td>\n",
       "      <td>51.070000</td>\n",
       "      <td>51.790001</td>\n",
       "      <td>50.959999</td>\n",
       "      <td>51.625000</td>\n",
       "      <td>51.117813</td>\n",
       "      <td>110481600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8/19/2019</td>\n",
       "      <td>52.654999</td>\n",
       "      <td>53.182499</td>\n",
       "      <td>52.507500</td>\n",
       "      <td>52.587502</td>\n",
       "      <td>52.070862</td>\n",
       "      <td>97654400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8/20/2019</td>\n",
       "      <td>52.720001</td>\n",
       "      <td>53.337502</td>\n",
       "      <td>52.580002</td>\n",
       "      <td>52.590000</td>\n",
       "      <td>52.073338</td>\n",
       "      <td>107537200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8/21/2019</td>\n",
       "      <td>53.247501</td>\n",
       "      <td>53.412498</td>\n",
       "      <td>52.900002</td>\n",
       "      <td>53.160000</td>\n",
       "      <td>52.637737</td>\n",
       "      <td>86141600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8/22/2019</td>\n",
       "      <td>53.297501</td>\n",
       "      <td>53.610001</td>\n",
       "      <td>52.687500</td>\n",
       "      <td>53.115002</td>\n",
       "      <td>52.593182</td>\n",
       "      <td>89014800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8/23/2019</td>\n",
       "      <td>52.357498</td>\n",
       "      <td>53.012501</td>\n",
       "      <td>50.250000</td>\n",
       "      <td>50.660000</td>\n",
       "      <td>50.162300</td>\n",
       "      <td>187272000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8/26/2019</td>\n",
       "      <td>51.465000</td>\n",
       "      <td>51.797501</td>\n",
       "      <td>51.264999</td>\n",
       "      <td>51.622501</td>\n",
       "      <td>51.115337</td>\n",
       "      <td>104174400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8/27/2019</td>\n",
       "      <td>51.965000</td>\n",
       "      <td>52.137501</td>\n",
       "      <td>50.882500</td>\n",
       "      <td>51.040001</td>\n",
       "      <td>50.538563</td>\n",
       "      <td>103493200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8/28/2019</td>\n",
       "      <td>51.025002</td>\n",
       "      <td>51.430000</td>\n",
       "      <td>50.830002</td>\n",
       "      <td>51.382500</td>\n",
       "      <td>50.877701</td>\n",
       "      <td>63755200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8/29/2019</td>\n",
       "      <td>52.125000</td>\n",
       "      <td>52.330002</td>\n",
       "      <td>51.665001</td>\n",
       "      <td>52.252499</td>\n",
       "      <td>51.739151</td>\n",
       "      <td>83962000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8/30/2019</td>\n",
       "      <td>52.540001</td>\n",
       "      <td>52.612499</td>\n",
       "      <td>51.799999</td>\n",
       "      <td>52.185001</td>\n",
       "      <td>51.672314</td>\n",
       "      <td>84573600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date       Open       High        Low      Close  Adj Close  \\\n",
       "0    8/1/2019  53.474998  54.507500  51.685001  52.107498  51.400280   \n",
       "1    8/2/2019  51.382500  51.607498  50.407501  51.005001  50.312748   \n",
       "2    8/5/2019  49.497501  49.662498  48.145000  48.334999  47.678982   \n",
       "3    8/6/2019  49.077499  49.517502  48.509998  49.250000  48.581562   \n",
       "4    8/7/2019  48.852501  49.889999  48.455002  49.759998  49.084641   \n",
       "5    8/8/2019  50.049999  50.882500  49.847500  50.857498  50.167248   \n",
       "6    8/9/2019  50.325001  50.689999  49.822498  50.247501  49.753849   \n",
       "7   8/12/2019  49.904999  50.512501  49.787498  50.119999  49.627602   \n",
       "8   8/13/2019  50.255001  53.035000  50.119999  52.242500  51.729252   \n",
       "9   8/14/2019  50.790001  51.610001  50.647499  50.687500  50.189526   \n",
       "10  8/15/2019  50.865002  51.285000  49.917500  50.435001  49.939507   \n",
       "11  8/16/2019  51.070000  51.790001  50.959999  51.625000  51.117813   \n",
       "12  8/19/2019  52.654999  53.182499  52.507500  52.587502  52.070862   \n",
       "13  8/20/2019  52.720001  53.337502  52.580002  52.590000  52.073338   \n",
       "14  8/21/2019  53.247501  53.412498  52.900002  53.160000  52.637737   \n",
       "15  8/22/2019  53.297501  53.610001  52.687500  53.115002  52.593182   \n",
       "16  8/23/2019  52.357498  53.012501  50.250000  50.660000  50.162300   \n",
       "17  8/26/2019  51.465000  51.797501  51.264999  51.622501  51.115337   \n",
       "18  8/27/2019  51.965000  52.137501  50.882500  51.040001  50.538563   \n",
       "19  8/28/2019  51.025002  51.430000  50.830002  51.382500  50.877701   \n",
       "20  8/29/2019  52.125000  52.330002  51.665001  52.252499  51.739151   \n",
       "21  8/30/2019  52.540001  52.612499  51.799999  52.185001  51.672314   \n",
       "\n",
       "       Volume  \n",
       "0   216071600  \n",
       "1   163448400  \n",
       "2   209572000  \n",
       "3   143299200  \n",
       "4   133457600  \n",
       "5   108038000  \n",
       "6    98478800  \n",
       "7    89927600  \n",
       "8   188874000  \n",
       "9   146189600  \n",
       "10  108909600  \n",
       "11  110481600  \n",
       "12   97654400  \n",
       "13  107537200  \n",
       "14   86141600  \n",
       "15   89014800  \n",
       "16  187272000  \n",
       "17  104174400  \n",
       "18  103493200  \n",
       "19   63755200  \n",
       "20   83962000  \n",
       "21   84573600  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the real stock price for February 1st 2012 - \n",
    "# January 31st 2017\n",
    "\n",
    "dataset_test = pd.read_csv('test_data.csv')\n",
    "dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_stock_price = dataset_test.iloc[:, 1:2].values\n",
    "real_stock_price.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[53.474998],\n",
       "       [51.3825  ],\n",
       "       [49.497501],\n",
       "       [49.077499],\n",
       "       [48.852501],\n",
       "       [50.049999],\n",
       "       [50.325001],\n",
       "       [49.904999],\n",
       "       [50.255001],\n",
       "       [50.790001],\n",
       "       [50.865002],\n",
       "       [51.07    ],\n",
       "       [52.654999],\n",
       "       [52.720001],\n",
       "       [53.247501],\n",
       "       [53.297501],\n",
       "       [52.357498],\n",
       "       [51.465   ],\n",
       "       [51.965   ],\n",
       "       [51.025002],\n",
       "       [52.125   ],\n",
       "       [52.540001]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_stock_price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the stock price value for each day in January 2017, we need the values in the last 60 days.\n",
    "\n",
    "To obtain this **history** we need to combine both the training and test sets in one.\n",
    "\n",
    "If we were to use the training_set and test_set we would need to use the scaler  but that would change the actual test values.  Thus concatenate the original data frames!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predicted stock price of 2017\n",
    "\n",
    "# axis = 0 means concatenate the lines (i.e. vertical axis)\n",
    "dataset_total = pd.concat((dataset_train['Open'], dataset_test['Open']), axis = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2559"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  This feature will simplify checks/tests to ensure data credibility.\n",
    "dataset_total.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the difference in the length of the first two gives us \n",
    "# the first day in 2017, and we need to go back 90 days to get the necessary range\n",
    "inputs = dataset_total[len(dataset_total) - len(dataset_test) - 90:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # To specify the maximum number of characters.\n",
    "inputs.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we did not use iloc from panda so lets reshape the numpy array for \n",
    "# compatibility: i.e. all the values from input lines to be stacked in one \n",
    "# column. The -1 means that the numpy has no knowledge of how the \n",
    "# values were stored in lines. The 1 means we want to them in one \n",
    "# column.\n",
    "\n",
    "inputs = inputs.reshape(-1,1) \n",
    "\n",
    "# apply the feature scaler\n",
    "inputs = sc.transform(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For each price in Jan. 2017 we need the **immediate 60 values** before it. \n",
    "2. We have 21 prices in January;\n",
    "3. We need a numpy 3D array of 60 prices (columns) times 21 days (rows) times 1 dependent variable \n",
    "4. We don’t need y_test. That is what we are trying to compute!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predicted stock price of 2017\n",
    "X_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first 90 from inputs are from training set; start \n",
    "# from 90 and get the extra 20, i.e. up to 80\n",
    "for i in range(90, 111): \n",
    "    X_test.append(inputs[i-90:i, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(X_test) # not 3D structure yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 3D structure\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_stock_price = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to inverse the scaling to get meaningful predicted stock price # outputs\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price) \n",
    "predicted_stock_price.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualising the results\n",
    "\n",
    "plt.plot(real_stock_price, color = 'pink', label = 'Real Google Stock Price')\n",
    "\n",
    "plt.plot(predicted_stock_price, color = 'green', label = 'Predicted Google Stock Price')\n",
    "\n",
    "plt.title('Google Stock Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Google Stock Price')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blue line shows the trend of the stock for the month of January 2017. \n",
    "\n",
    "Some observations:\n",
    "- The prediction lags behind the actual price curve because the model cannot react to fast non-linear changes. Spikes are examples of fast non-linear changes\n",
    "- Model reacts pretty well to smooth changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the RMSE\n",
    "\n",
    "If we need to compute the RMSE for our Stock Price Prediction problem, we use the real stock price and predicted stock price as shown.\n",
    "\n",
    "Then consider dividing this RMSE by the range of the Google Stock Price values of January 2017 to get a relative error, as opposed to an absolute error. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.86817123051945"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse = math.sqrt( mean_squared_error( real_stock_price[0:21,:], predicted_stock_price))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new data need to be placed in the same order/format  as in the case of the training/test sets.\n",
    "\n",
    "1. Getting more training data: we trained our model on the past 5 years of the  Google Stock Price but it would be even better to train it on the past 10 years.\n",
    "\n",
    "2. Increasing the number of time steps: the model remembered the stock price from the 60 previous financial days to predict the stock price of the next day. That’s because we chose a number of 60 time steps (3 months). You could try to increase the number of time steps, by choosing for example 120 time steps (6 months).\n",
    "\n",
    "3. Adding some other indicators: if you have the financial instinct that the stock price of some other companies might be correlated to the one of Google, you could add this other stock price as a new indicator in the training data.\n",
    "\n",
    "4. Adding more LSTM layers: we built a RNN with four LSTM layers but you could try with even more.\n",
    "\n",
    "5. Adding more neurons in the LSTM layers: we highlighted the fact that we needed a high number of neurons in the LSTM layers to respond better to the complexity of the problem and we chose to include 50 neurons in each of our 4 LSTM layers. You could try an architecture with even more neurons in each of the 4 (or more) LSTM layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning the RNN\n",
    "\n",
    "Parameter Tuning on the RNN model: we are dealing with a Regression problem because we predict a continuous outcome.\n",
    "\n",
    "**Tip**: replace: scoring = 'accuracy' by scoring = 'neg_mean_squared_error' in the GridSearchCV class parameters as we did in the ANN case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
